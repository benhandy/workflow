{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO1VLlbWX6xd9C+4BKmqSQ0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/benhandy/workflow/blob/main/Feature_Engineering_for_ML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "id": "EKGSUT6xWQd4",
        "outputId": "799be209-151e-4fe5-83a1-e0f6cc1f4070"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nFeature Engineering Lab: Building ML-Ready Datasets\\n\\nIn this lab, I'll walk through the process of feature engineering in three steps. First is data extraction, where I'll connect to various data sources, manage large data volumes, and extract the dataset needed to train the ML model. This step sets the foundation for everything that follows. \\n\\nNext is feature creation, where I'll transform the extracted data into numerical features that the model can learn from. This involves labeling records to train the model, generating vector embeddings to represent text data, and encoding categorical values into numerical formats.\\n\\nFinally, I'll focus on feature storage. After cleaning and transforming the data, I'll store the final dataset in an accessible system for the ML team. This step includes splitting the data into training and testing sets and saving them in S3 buckets for further exploration and model training.\\n\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "\"\"\"\n",
        "Feature Engineering Lab: Building ML-Ready Datasets\n",
        "\n",
        " In this lab, I'll walk through the process of feature engineering in three steps:\n",
        "\n",
        "  First is data extraction, where I'll connect to various data sources, manage large data volumes, and extract the dataset needed to train the ML model.\n",
        "\n",
        "   Next is feature creation, where I'll transform the extracted data into numerical features that the model can learn from.\n",
        "    This involves labeling records to train the model, generating vector embeddings to represent text data, and encoding categorical values into numerical formats.\n",
        "\n",
        "     Finally, I'll focus on feature storage. After cleaning and transforming the data, I'll store the final dataset in an accessible system for the ML team.\n",
        "      This step includes splitting the data into training and testing sets and saving them in S3 buckets for further exploration and model training.\n",
        "\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import io\n",
        "import datetime as dt\n",
        "import pickle\n",
        "\n",
        "import awswrangler as wr\n",
        "import boto3\n",
        "from dotenv import load_dotenv\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pyarrow as pa\n",
        "import pyarrow.parquet as pq\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder, StandardScaler, KBinsDiscretizer\n",
        "\n",
        "%load_ext sql"
      ],
      "metadata": {
        "id": "pu53Lb9OWg9V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%config SqlMagic.style = '_DEPRECATED_DEFAULT'"
      ],
      "metadata": {
        "id": "wOLxmiNgWhHg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BUCKET_NAME = 'de-c4w2lab1-339712961490-us-east-1-data-bucket'"
      ],
      "metadata": {
        "id": "zmIF3MMOWypu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# getting endpoint in AWS bash terminal\n",
        "# this is a bash command\n",
        "aws rds describe-db-instances --db-instance-identifier de-c4w2lab1-rds --output text --query \"DBInstances[].Endpoint.Address\"\n"
      ],
      "metadata": {
        "id": "uVEzg_UmWyuI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# updating env file with endpoint address\n",
        "DBHOST=de-c4w2lab1-rds.cfe0es80i889.us-east-1.rds.amazonaws.com\n",
        "DBUSER = admin\n",
        "DBPASSWORD = adminpwrd\n",
        "DBPORT = 3306\n",
        "DBNAME = classicmodels"
      ],
      "metadata": {
        "id": "fTRix416WzFb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loading env variables and connecting to database\n",
        "load_dotenv('./src/env', override=True)\n",
        "\n",
        "DBHOST = os.getenv('DBHOST')\n",
        "DBPORT = os.getenv('DBPORT')\n",
        "DBNAME = os.getenv('DBNAME')\n",
        "DBUSER = os.getenv('DBUSER')\n",
        "DBPASSWORD = os.getenv('DBPASSWORD')\n",
        "\n",
        "connection_url = f\"mysql+pymysql://{DBUSER}:{DBPASSWORD}@{DBHOST}:{DBPORT}/{DBNAME}\"\n",
        "\n",
        "%sql {connection_url}"
      ],
      "metadata": {
        "id": "p9VnAVrvW0_x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sql command to test database connection\n",
        "%%sql\n",
        "use classicmodels;\n",
        "show tables;\n",
        "\n",
        "# examine ratings table\n",
        "SELECT *\n",
        "FROM ratings\n",
        "LIMIT 10;"
      ],
      "metadata": {
        "id": "tKJJ9vp5W1GZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now I'll create a new base dataset for the ML model by extracting the required data from different tables. Using this initial dataset, I'll add new features or enhance the ones I've selected. I'll also use the schema from the transformed table to define and create a new table."
      ],
      "metadata": {
        "id": "JSU0PMZ-X_NP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "I'll open the file ./src/de-c4w2lab1-etl-glue-job.py. This file contains the code for the AWS Glue Job.\n",
        " In the job, I created a node to pull the required data from each table in classicmodels using the Glue Catalog.\n",
        "  Then, I used a SQL query to join the tables, and the result of that join was stored in S3.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# This is the ./src/de-c4w2lab1-etl-glue-job.py file\n",
        "\n",
        "import sys\n",
        "\n",
        "from awsglue import DynamicFrame\n",
        "from awsglue.context import GlueContext\n",
        "from awsglue.job import Job\n",
        "from awsglue.transforms import *\n",
        "from awsglue.utils import getResolvedOptions\n",
        "from pyspark.context import SparkContext\n",
        "\n",
        "\n",
        "def sparkSqlQuery(\n",
        "    glueContext, query, mapping, transformation_ctx\n",
        ") -> DynamicFrame:\n",
        "    for alias, frame in mapping.items():\n",
        "        frame.toDF().createOrReplaceTempView(alias)\n",
        "    result = spark.sql(query)\n",
        "    return DynamicFrame.fromDF(result, glueContext, transformation_ctx)\n",
        "\n",
        "\n",
        "args = getResolvedOptions(\n",
        "    sys.argv, [\"JOB_NAME\", \"glue_connection\", \"glue_database\", \"target_path\"]\n",
        ")\n",
        "sc = SparkContext()\n",
        "glueContext = GlueContext(sc)\n",
        "spark = glueContext.spark_session\n",
        "job = Job(glueContext)\n",
        "job.init(args[\"JOB_NAME\"], args)\n",
        "\n",
        "# script generated for node Products\n",
        "products_node = glueContext.create_dynamic_frame.from_options(\n",
        "    connection_type=\"mysql\",\n",
        "    connection_options={\n",
        "        \"useConnectionProperties\": \"true\",\n",
        "        \"dbtable\": \"classicmodels.products\",\n",
        "        \"connectionName\": args[\"glue_connection\"],\n",
        "    },\n",
        "    transformation_ctx=\"products_node\",\n",
        ")\n",
        "\n",
        "# Script generated for node Customers\n",
        "customers_node = glueContext.create_dynamic_frame.from_options(\n",
        "    connection_type=\"mysql\",\n",
        "    connection_options={\n",
        "        \"useConnectionProperties\": \"true\",\n",
        "        \"dbtable\": \"classicmodels.customers\",\n",
        "        \"connectionName\": args[\"glue_connection\"],\n",
        "    },\n",
        "    transformation_ctx=\"customers_node\",\n",
        ")\n",
        "\n",
        "# script generated for node Ratings\n",
        "ratings_node = glueContext.create_dynamic_frame.from_options(\n",
        "    connection_type=\"mysql\",\n",
        "    connection_options={\n",
        "        \"useConnectionProperties\": \"true\",\n",
        "        \"dbtable\": \"classicmodels.ratings\",\n",
        "        \"connectionName\": args[\"glue_connection\"],\n",
        "    },\n",
        "    transformation_ctx=\"ratings_node\",\n",
        ")\n",
        "\n",
        "# script generated for node Join\n",
        "sql_join_query = \"\"\"\n",
        "select r.customerNumber\n",
        ", c.city\n",
        ", c.state\n",
        ", c.postalCode\n",
        ", c.country\n",
        ", c.creditLimit\n",
        ", r.productCode\n",
        ", p.productLine\n",
        ", p.productScale\n",
        ", p.quantityInStock\n",
        ", p.buyPrice\n",
        ", p.MSRP\n",
        ", r.productRating\n",
        "from ratings r\n",
        "join products p on p.productCode = r.productCode\n",
        "join customers c on c.customerNumber = r.customerNumber;\n",
        "\"\"\"\n",
        "\n",
        "join_node = sparkSqlQuery(\n",
        "    glueContext,\n",
        "    query=sql_join_query,\n",
        "    mapping={\n",
        "        \"ratings\": ratings_node,\n",
        "        \"products\": products_node,\n",
        "        \"customers\": customers_node,\n",
        "    },\n",
        "    transformation_ctx=\"join_node\",\n",
        ")\n",
        "\n",
        "# script generated for node de-c1w4-s3\n",
        "s3_upload_node = glueContext.getSink(\n",
        "    path=f\"{args['target_path']}/ratings_ml_training/\",\n",
        "    connection_type=\"s3\",\n",
        "    updateBehavior=\"UPDATE_IN_DATABASE\",\n",
        "    partitionKeys=[\"customerNumber\"],\n",
        "    enableUpdateCatalog=True,\n",
        "    transformation_ctx=\"s3_upload_node\",\n",
        ")\n",
        "s3_upload_node.setCatalogInfo(\n",
        "    catalogDatabase=args[\"glue_database\"],\n",
        "    catalogTableName=\"ratings_ml_training\",\n",
        ")\n",
        "s3_upload_node.setFormat(\"glueparquet\", compression=\"snappy\")\n",
        "s3_upload_node.writeFrame(join_node)\n",
        "job.commit()\n"
      ],
      "metadata": {
        "id": "UGAgslapWzU3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creating a pandas dataframe and displaying it\n",
        "sql_join_query = \"\"\"\n",
        "select r.customerNumber\n",
        ", c.city\n",
        ", c.state\n",
        ", c.postalCode\n",
        ", c.country\n",
        ", c.creditLimit\n",
        ", r.productCode\n",
        ", p.productLine\n",
        ", p.productScale\n",
        ", p.quantityInStock\n",
        ", p.buyPrice\n",
        ", p.MSRP\n",
        ", r.productRating\n",
        ", c.customerSince\n",
        "from ratings r\n",
        "join products p on p.productCode = r.productCode\n",
        "join customers c on c.customerNumber = r.customerNumber;\n",
        "\"\"\"\n",
        "\n",
        "result = %sql {sql_join_query}\n",
        "\n",
        "model_df = result.DataFrame()\n",
        "\n",
        "model_df.head()"
      ],
      "metadata": {
        "id": "StkYwDTSWzbr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creating a basic train/test split of the dataset\n",
        "model_train_df, model_test_df = train_test_split(\n",
        "                model_df,\n",
        "                test_size=0.2,\n",
        "                random_state=42\n",
        "            )"
      ],
      "metadata": {
        "id": "nhYGDjotWhQX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_train_df.head()"
      ],
      "metadata": {
        "id": "Sb1nXsPLY_Wj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# to create the required features based on the initial sub-datasets, I'll have to distinguish between numerical and categorical variables\n",
        "# first i am converting column names to lowercase to avoid problems down the line\n",
        "model_train_df.columns = [col.lower() for col in model_train_df.columns]"
      ],
      "metadata": {
        "id": "6_Q10FWkY_cv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creating the users dataframe\n",
        "user_columns = [\"customernumber\", \"city\", \"country\", \"creditlimit\", \"customersince\"]\n",
        "users_df = model_train_df[user_columns].copy()\n",
        "\n",
        "# creating the items dataframe\n",
        "item_columns = [\"productcode\", \"productline\", \"productscale\", \"quantityinstock\", \"buyprice\", \"msrp\"]\n",
        "items_df = model_train_df[item_columns].copy()"
      ],
      "metadata": {
        "id": "SCbOPv-ZY_iw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "users_df.head()"
      ],
      "metadata": {
        "id": "bZaS8o3KY_o3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "items_df.head()"
      ],
      "metadata": {
        "id": "5iX1VjsLY_u5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now i am feature engineering for numerical variables. To standardize my numerical data, I create a StandardScaler instance, fit the scaler to the data, and transform the data."
      ],
      "metadata": {
        "id": "tcj1Y84laWN3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# instantiating the StandardScaler object and assign it to a variable\n",
        "\n",
        "# StandardScaler instance for the user numerical data\n",
        "user_num_standard_scaler = StandardScaler()\n",
        "\n",
        "# StandardScaler instance for the item numerical data\n",
        "item_num_standard_scaler = StandardScaler()\n"
      ],
      "metadata": {
        "id": "m6dLTKHWY_0t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# using the fit() method over the data\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# choosing the numerical columns to apply the standard scaler in the two datasets\n",
        "user_num_columns_std = [\"creditlimit\"]\n",
        "item_num_columns_std = [\"quantityinstock\", \"buyprice\", \"msrp\"]\n",
        "\n",
        "# computing mean and standard deviation of each feature with the fit method\n",
        "user_num_standard_scaler = StandardScaler().fit(users_df[user_num_columns_std])\n",
        "item_num_standard_scaler = StandardScaler().fit(items_df[item_num_columns_std])\n",
        "\n"
      ],
      "metadata": {
        "id": "M7OWfiHNamWQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "I will apply the transform() method to both user_num_standard_scaler and item_num_standard_scaler\n",
        " I need to pass the same DataFrames, users_df and items_df, along with their respective selected columns lists:\n",
        "  user_num_columns_std for users_df and item_num_columns_std for items_df.\n",
        "   The results of transform() will be numpy arrays, so I will convert them back into pandas DataFrames.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# applying transform\n",
        "user_num_vars_std = user_num_standard_scaler.transform(users_df[user_num_columns_std])\n",
        "item_num_vars_std = item_num_standard_scaler.transform(items_df[item_num_columns_std])\n",
        "\n",
        "\n",
        "# results are numpy arrays, now i transform them into pandas dataframes\n",
        "user_num_vars_std_df = pd.DataFrame(user_num_vars_std, columns=user_num_columns_std, index=users_df.index)\n",
        "item_num_vars_std_df = pd.DataFrame(item_num_vars_std, columns=item_num_columns_std, index=items_df.index)\n",
        "\n"
      ],
      "metadata": {
        "id": "66l6hgDtamfQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "item_num_vars_std_df.head()"
      ],
      "metadata": {
        "id": "p4_EJHWbamnm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_num_vars_std_df.head()"
      ],
      "metadata": {
        "id": "3FuXacobamu_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Before encoding categorical variables, I'll extract and clean the features. I'll create two lists:\n",
        "\n",
        "*   user_cat_cols for the categorical variables in users_df, which include city and country.\n",
        "\n",
        "*   item_cat_cols for items_df, which include productline and productscale."
      ],
      "metadata": {
        "id": "lL7GscCGcVaR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# column names for the categorical variables\n",
        "user_cat_cols = [\"city\", \"country\"]\n",
        "item_cat_cols = [\"productline\", \"productscale\"]"
      ],
      "metadata": {
        "id": "uB0i-VzXam2y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating the datasets with only categorical features\n",
        "users_cat_df = users_df[user_cat_cols].copy()\n",
        "items_cat_df = items_df[item_cat_cols].copy()\n",
        "\n",
        "# Converting string categories into lowercase\n",
        "users_cat_df = users_cat_df.apply(lambda col: col.map(lambda x: x.strip().lower() if isinstance(x, str) else x))\n",
        "items_cat_df = items_cat_df.apply(lambda col: col.map(lambda x: x.strip().lower() if isinstance(x, str) else x))\n"
      ],
      "metadata": {
        "id": "mztFwZLBam96"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The features I've selected are categorical, so I’ll use scikit-learn’s OneHotEncoder to create a one-hot encoded array. By default, the encoder derives categories from unique values, but I can also pass them manually. First, I’ll instantiate the OneHotEncoder object and set handle_unknown=\"ignore\", which ensures that unknown categories are encoded as all zeros."
      ],
      "metadata": {
        "id": "RzRSk0bUdHxB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create the instance of the one-hot encoder object for each dataset.\n",
        "user_cat_ohe = OneHotEncoder(handle_unknown=\"ignore\")\n",
        "item_cat_ohe = OneHotEncoder(handle_unknown=\"ignore\")\n",
        "\n"
      ],
      "metadata": {
        "id": "QWhu9xMOcnDV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# passing the DataFrames users_cat_df and items_cat_df to perform the transformation\n",
        "# then converting it into the dense matrix with the todense() method\n",
        "# using the encoder object to find the categories of each feature\n",
        "user_cat_ohe.fit(users_cat_df)\n",
        "item_cat_ohe.fit(items_cat_df)\n",
        "\n",
        "# transform with the encoder objects\n",
        "encoded_user_cat_features = user_cat_ohe.transform(users_cat_df).todense()\n",
        "encoded_item_cat_features = item_cat_ohe.transform(items_cat_df).todense()"
      ],
      "metadata": {
        "id": "Mjyf8nPTcnJ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_user_cat_df = pd.DataFrame(\n",
        "    encoded_user_cat_features,\n",
        "    columns=user_cat_ohe.get_feature_names_out(user_cat_cols),\n",
        "    index=users_df.index\n",
        ")\n",
        "\n",
        "encoded_item_cat_df = pd.DataFrame(\n",
        "    encoded_item_cat_features,\n",
        "    columns=item_cat_ohe.get_feature_names_out(item_cat_cols),\n",
        "    index=items_df.index\n",
        ")"
      ],
      "metadata": {
        "id": "uumf3OjNcnRZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_user_cat_df.head()"
      ],
      "metadata": {
        "id": "DIZdOf_NcnYI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_item_cat_df.head()"
      ],
      "metadata": {
        "id": "tv_hbPnncnf9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now I create bins for the customers given their antiquity with the company in the following way:\n",
        "\n",
        "-A bin for the most recent customers, with 1 year or less of antiquity\n",
        "\n",
        "-A bin for customers with an antiquity greater than 1 but less than 3 years\n",
        "\n",
        "-A bin for customers with antiquity of 3-5 years\n",
        "\n",
        "-A bin for customers with more than 5 years of antiquity\n",
        "\n",
        "In order to generate the binning process, first I have to create the loyalty_program_years column based on the date provided in the customersince column."
      ],
      "metadata": {
        "id": "Is2OEABGd1RA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import KBinsDiscretizer\n",
        "\n",
        "# define bin edges\n",
        "bin_edges = [0, 1, 3, 5, float('inf')]\n",
        "\n",
        "# converting the `customersince` column of the dataframe `users_df` to datetime data type with `pd.to_datetime()` method\n",
        "users_df['customersince'] = pd.to_datetime(users_df['customersince'])\n",
        "\n",
        "# compute current timestamp with `pd.Timestamp.now()`\n",
        "current_date = pd.Timestamp.now()\n",
        "\n",
        "# create the column with the difference between the `current_date` value and the `customersince` column of the `users_df` dataframe\n",
        "# convert the value in days by applying `dt.days` method and make an integer division by 365\n",
        "users_df['loyalty_program_years'] = (current_date - users_df['customersince']).dt.days // 365\n",
        "\n",
        "\"\"\"\n",
        "Initialize the `KBinsDiscretizer` class with the following parameters:\n",
        "Set `n_bins` as `len(bin_edges) - 1` because the number of bins is always one less than the number of edges.\n",
        "Set `encode` equal to `'onehot-dense'`. This specifies how to encode the transformed result. Here the strategy is to apply to the bins a one-hot encoding.\n",
        "Set `strategy` parameter equal to `'uniform'` which is a binning strategy where bins are of equal width.\n",
        "Leave `subsample` equal to `None` as it is; this option means that all the training samples are used when computing the quantiles that determine the binning thresholds.\n",
        "\"\"\"\n",
        "kbins = KBinsDiscretizer(n_bins=len(bin_edges) - 1, encode='onehot-dense', strategy='uniform', subsample=None)\n",
        "\n",
        "# selecting `loyalty_program_years` from the dataframe `users_df`. Use double brackets to output it as a dataframe, not a series\n",
        "loyalty_program_years = users_df[['loyalty_program_years']]\n",
        "\n",
        "# applying `fit()` method to the dataframe `loyalty_program_years`\n",
        "kbins.fit(loyalty_program_years)\n",
        "\n",
        "# applying `transform()` method to transform to the dataframe `loyalty_program_years`\n",
        "loyalty_program_years_binned = kbins.transform(loyalty_program_years)\n",
        "\n",
        "# converting the binned data to a DataFrame with appropriate column names\n",
        "bin_labels = ['0-1 years', '1-3 years', '3-5 years', '5+ years']\n",
        "loyalty_program_years_binned_df = pd.DataFrame(loyalty_program_years_binned, columns=bin_labels, index=users_df.index)\n",
        "\n",
        "loyalty_program_years_binned_df.head()\n"
      ],
      "metadata": {
        "id": "wM_IZBP2ZAVd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the recommendation system, the label is the rating (1 to 5) a user gives to products. I’ll treat these ratings as categories and use MinMaxScaler to scale them from [1, 5] to [-1, 1] for better compatibility with ML models."
      ],
      "metadata": {
        "id": "5uRzpFTLe9uh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# creating an instance of MinMaxScaler for ratings\n",
        "rating_scaler = MinMaxScaler(feature_range=(-1, 1))"
      ],
      "metadata": {
        "id": "H4dac9avdtf2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# apply `fit()` method\n",
        "rating_scaler.fit(model_train_df[[\"productrating\"]])\n",
        "\n",
        "# perform transformation\n",
        "ratings = rating_scaler.transform(model_train_df[[\"productrating\"]])\n",
        "\n",
        "\n",
        "scaled_ratings_df = pd.DataFrame(ratings, columns=[\"scaled_productrating\"], index=model_train_df.index)\n",
        "scaled_ratings_df.head()\n"
      ],
      "metadata": {
        "id": "nsZsgnFpdtmn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create the dataset of transformed features from the training data, using the panda's concat() method over the axis=1\n",
        "transformed_train_df = pd.concat([\n",
        "                                    users_df[[\"customernumber\"]],\n",
        "                                    user_num_vars_std_df,\n",
        "                                    encoded_user_cat_df,\n",
        "                                    items_df[[\"productcode\"]],\n",
        "                                    item_num_vars_std_df,\n",
        "                                    encoded_item_cat_df,\n",
        "                                    loyalty_program_years_binned_df,\n",
        "                                    scaled_ratings_df\n",
        "                                ],\n",
        "                                axis=1\n",
        "                                )\n",
        "\n",
        "transformed_train_df.head()"
      ],
      "metadata": {
        "id": "d2d3-V_9dttk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With this joined dataset, I can safely store it in an S3 bucket. In addition to the dataset, I also need to store the artifacts from my scaler and encoder objects. These are crucial because they've been applied to the training data, and I’ll need the computed values to transform the test data or use the model for inference later."
      ],
      "metadata": {
        "id": "1V99UkaGfnsM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# uploading the artifacts and then i will insert the data\n",
        "s3 = boto3.client('s3')\n",
        "\n",
        "artifacts_folder = 'preprocessing/artifacts'\n",
        "\n",
        "# standard scaler for users\n",
        "user_num_std_scaler_pkl = pickle.dumps(user_num_standard_scaler)\n",
        "s3.put_object(Bucket=BUCKET_NAME, Key=f'{artifacts_folder}/user_std_scaler.pkl', Body=user_num_std_scaler_pkl)\n",
        "\n",
        "# standard scaler for items\n",
        "item_num_std_scaler_pkl = pickle.dumps(item_num_standard_scaler)\n",
        "s3.put_object(Bucket=BUCKET_NAME, Key=f'{artifacts_folder}/item_std_scaler.pkl', Body=item_num_std_scaler_pkl)\n",
        "\n",
        "# binnerizer for user's years with company\n",
        "kbins_pkl = pickle.dumps(kbins)\n",
        "s3.put_object(Bucket=BUCKET_NAME, Key=f'{artifacts_folder}/user_kbins.pkl', Body=kbins_pkl)\n",
        "\n",
        "# standard scaler for users\n",
        "user_cat_ohe_pkl = pickle.dumps(user_cat_ohe)\n",
        "s3.put_object(Bucket=BUCKET_NAME, Key=f'{artifacts_folder}/user_cat_ohe.pkl', Body=user_cat_ohe_pkl)\n",
        "\n",
        "# standard scaler for items\n",
        "item_cat_ohe_pkl = pickle.dumps(item_cat_ohe)\n",
        "s3.put_object(Bucket=BUCKET_NAME, Key=f'{artifacts_folder}/item_cat_ohe.pkl', Body=item_cat_ohe_pkl)\n",
        "\n",
        "# scaler for ratings\n",
        "rating_scaler_pkl = pickle.dumps(rating_scaler)\n",
        "s3.put_object(Bucket=BUCKET_NAME, Key=f'{artifacts_folder}/ratings_min_max_scaler.pkl', Body=rating_scaler_pkl)\n"
      ],
      "metadata": {
        "id": "NBi14ZFhdt0Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save data\n",
        "data_s3_url = f's3://{BUCKET_NAME}/preprocessing/data/ratings_for_ml/train'\n",
        "\n",
        "transformed_train_df.to_parquet(data_s3_url,\n",
        "                                compression='snappy',\n",
        "                                engine='pyarrow',\n",
        "                                partition_cols=['productcode'],\n",
        "                                existing_data_behavior='delete_matching'\n",
        "                            )\n"
      ],
      "metadata": {
        "id": "cl60sXxQdt7R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# view in AWS S3 bucket\n",
        "!aws s3 ls s3://$BUCKET_NAME/preprocessing/data/ratings_for_ml/train/"
      ],
      "metadata": {
        "id": "PzPM4kTiduCL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# read back the data\n",
        "train_data = pd.read_parquet(data_s3_url)\n",
        "train_data.head()"
      ],
      "metadata": {
        "id": "mXskyxcxduIx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Until now, I’ve only worked on transforming my training data. When I need to test my model or perform inference, I’ll transform the incoming data using the same steps I followed for the training data. The only difference is that I don’t need to apply the fit() method to the scalers or encoders. I only use the transform() method on the test or inference data since the fit() method is for computing statistics from the training data, and I don’t need to recalculate them for testing."
      ],
      "metadata": {
        "id": "6yU8FdpMgTXK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# now i replicate the steps that i followed for training data to transform the test DataFrame model_test_df\n",
        "# convert the column names from the `model_test_df` dataframe into lowercase\n",
        "model_test_df.columns = [col.lower() for col in model_test_df.columns]\n",
        "\n",
        "# creating the users dataframe from `model_test_df` with the `copy()` method\n",
        "user_columns = [\"customernumber\", \"city\", \"country\", \"creditlimit\", \"customersince\"]\n",
        "users_test_df = model_test_df[user_columns].copy()\n",
        "\n",
        "# creating the items dataframe from `model_test_df` with the `copy()` method\n",
        "item_columns = [\"productcode\", \"productline\", \"productscale\", \"quantityinstock\", \"buyprice\", \"msrp\"]\n",
        "items_test_df = model_test_df[item_columns].copy()"
      ],
      "metadata": {
        "id": "8WelFGKvduPB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# use the transform() method of those objects and apply them to the test data (users_test_df and items_test_df\n",
        "\n",
        "# transforming the test data\n",
        "user_test_num_vars_std = user_num_standard_scaler.transform(users_test_df[user_num_columns_std])\n",
        "item_test_num_vars_std = item_num_standard_scaler.transform(items_test_df[item_num_columns_std])\n",
        "\n",
        "# results are numpy arrays which i will transform into pandas dataframes\n",
        "user_test_num_vars_std_df = pd.DataFrame(user_test_num_vars_std, columns=user_num_columns_std, index=users_test_df.index)\n",
        "item_test_num_vars_std_df = pd.DataFrame(item_test_num_vars_std, columns=item_num_columns_std, index=items_test_df.index)\n"
      ],
      "metadata": {
        "id": "CjZWQmV1gUCk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_test_num_vars_std_df.head()"
      ],
      "metadata": {
        "id": "V9E0tgi3gUIw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "item_test_num_vars_std_df.head()"
      ],
      "metadata": {
        "id": "cBXJx3UtgUPe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "selecting the categorical features from the test DataFrames using the column names provided.\n",
        "Then, I'll convert all the string values into lowercase.\n",
        "After that, I’ll use the transform() method to compute the one-hot encodings,\n",
        "and I’ll chain it with the todense() method to get the complete matrix.\n",
        "\"\"\"\n",
        "# column names for the categorical variables\n",
        "user_cat_cols = [\"city\", \"country\"]\n",
        "item_cat_cols = [\"productline\", \"productscale\"]\n",
        "\n",
        "# creating the datasets with only categorical features\n",
        "users_test_cat_df = users_test_df[user_cat_cols].copy()\n",
        "items_test_cat_df = items_test_df[item_cat_cols].copy()\n",
        "\n",
        "# converting string categories into lowercase\n",
        "users_test_cat_df = users_test_cat_df.map(lambda x: x.strip().lower())\n",
        "items_test_cat_df = items_test_cat_df.map(lambda x: x.strip().lower())\n",
        "\n",
        "# transform with the encoder objects\n",
        "encoded_user_test_cat_features = user_cat_ohe.transform(users_test_cat_df).todense()\n",
        "encoded_item_test_cat_features = item_cat_ohe.transform(items_test_cat_df).todense()\n",
        "\n",
        "encoded_user_test_cat_df = pd.DataFrame(\n",
        "    encoded_user_test_cat_features,\n",
        "    columns=user_cat_ohe.get_feature_names_out(user_cat_cols),\n",
        "    index=users_test_df.index\n",
        ")\n",
        "\n",
        "encoded_item_test_cat_df = pd.DataFrame(\n",
        "    encoded_item_test_cat_features,\n",
        "    columns=item_cat_ohe.get_feature_names_out(item_cat_cols),\n",
        "    index=items_test_df.index\n",
        ")\n"
      ],
      "metadata": {
        "id": "wzyXxkm6gUVl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_user_test_cat_df.head()"
      ],
      "metadata": {
        "id": "xdZvKh7RgUcA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_item_test_cat_df.head()"
      ],
      "metadata": {
        "id": "r5VBA4eMduVP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# applying the `transform()` method of the `KBinsDiscretizer` objects to the test data\n",
        "# i need to create the `loyalty_program_years` column to create the bins from it\n",
        "\n",
        "from sklearn.preprocessing import KBinsDiscretizer\n",
        "\n",
        "# define the bin edges\n",
        "bin_edges = [0, 1, 3, 5, float('inf')]\n",
        "\n",
        "# convert the `customersince` column of the dataframe `users_test_df` to datetime data type with `pd.to_datetime()` method\n",
        "users_test_df['customersince'] = pd.to_datetime(users_test_df['customersince'])\n",
        "\n",
        "# create the column with the difference between the `current_date` value and the `customersince` column of the `users_test_df` dataframe\n",
        "# convert the value in days by applying `dt.days` method and make an integer division by 365\n",
        "users_test_df['loyalty_program_years'] = (current_date - users_test_df['customersince']).dt.days // 365\n",
        "\n",
        "# select `loyalty_program_years` from the dataframe `users_test_df`. Use double brackets to output it as a dataframe, not a series\n",
        "loyalty_program_years_test = users_test_df[['loyalty_program_years']]\n",
        "\n",
        "# instantiate KBinsDiscretizer and apply `transform()` method to the `loyalty_program_years_test` dataframe\n",
        "kbins_discretizer = KBinsDiscretizer(n_bins=4, encode='onehot', strategy='uniform')\n",
        "loyalty_program_years_test_binned = kbins_discretizer.fit_transform(loyalty_program_years_test)\n",
        "\n",
        "# convert the binned data to a dataframe with appropriate column names\n",
        "bin_labels = ['0-1 years', '1-3 years', '3-5 years', '5+ years']\n",
        "loyalty_program_years_test_binned_df = pd.DataFrame(loyalty_program_years_test_binned.toarray(), columns=bin_labels, index=users_test_df.index)\n",
        "\n",
        "loyalty_program_years_test_binned_df.head()\n"
      ],
      "metadata": {
        "id": "uCq9jn71dubW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# performing transformation over the ratings\n",
        "ratings_test = rating_scaler.transform(model_test_df[[\"productrating\"]])\n",
        "\n",
        "scaled_ratings_test_df = pd.DataFrame(ratings_test, columns=[\"scaled_productrating\"], index=model_test_df.index)\n",
        "scaled_ratings_test_df.head()"
      ],
      "metadata": {
        "id": "5azoDneghjj1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# now i collect all my transformed DataFrames into one that will be saved into the S3 bucket\n",
        "transformed_test_df = pd.concat([\n",
        "                                    users_test_df[[\"customernumber\"]],\n",
        "                                    user_test_num_vars_std_df,\n",
        "                                    encoded_user_test_cat_df,\n",
        "                                    items_test_df[[\"productcode\"]],\n",
        "                                    item_test_num_vars_std_df,\n",
        "                                    encoded_item_test_cat_df,\n",
        "                                    scaled_ratings_test_df\n",
        "                                ],\n",
        "                                axis=1\n",
        "                                )\n",
        "\n",
        "transformed_test_df.head()"
      ],
      "metadata": {
        "id": "xAlmM5yzhjqL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# finally i save the transformed_test_df DataFrame into the S3 bucket\n",
        "data_s3_url = f's3://{BUCKET_NAME}/preprocessing/data/ratings_for_ml/test'\n",
        "\n",
        "transformed_test_df.to_parquet(data_s3_url,\n",
        "                                compression='snappy',\n",
        "                                engine='pyarrow',\n",
        "                                partition_cols=['productcode'],\n",
        "                                existing_data_behavior='delete_matching'\n",
        "                            )"
      ],
      "metadata": {
        "id": "1fCcd_ZOhjwE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "K_wnRt8Chj1-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}